{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import loader\n",
    "from threes import *\n",
    "from qLearningNet import *\n",
    "import time\n",
    "\n",
    "MOVES = [0, 1, 2, 3]\n",
    "CUDA = False\n",
    "\n",
    "FILENAME = \"saved_parameters\"\n",
    "INPUT_SIZE = 19\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.8\n",
    "\n",
    "\n",
    "# parametry sieci np. learning rate\n",
    "# lepszy zbior uczacy - jak w DeepMindzie\n",
    "# nagrody - musza byc wieksze - inaczej sami sobie redukujemy learning rate\n",
    "# (gestsze nagrody - np. logarytm z rewardow)\n",
    "# uczyc jednak dobrymi przebiegami\n",
    "# http://pytorch.org/docs/master/nn.html#embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model!\n",
      "Interrupt at any time to get current model\n",
      "Minibatch    100  | loss 203.813262939453 \n",
      "Minibatch    200  | loss 104.367820739746 \n",
      "Minibatch    300  | loss 73869.093750000000 \n",
      "Minibatch    400  | loss 649.117919921875 \n",
      "Minibatch    500  | loss 1622.707885742188 \n",
      "Minibatch    600  | loss 19754.361328125000 \n",
      "Minibatch    700  | loss 2482.094970703125 \n",
      "Minibatch    800  | loss 356.112457275391 \n",
      "Minibatch    900  | loss 26.915170669556 \n",
      "Minibatch   1000  | loss 2112.125732421875 \n",
      "Minibatch   1100  | loss 149995.171875000000 \n",
      "Minibatch   1200  | loss 21199.302734375000 \n",
      "Minibatch   1300  | loss 536.539245605469 \n",
      "Minibatch   1400  | loss 14.363482475281 \n",
      "Minibatch   1500  | loss  2.086886644363 \n",
      "Minibatch   1600  | loss 2054.673828125000 \n",
      "Minibatch   1700  | loss 596.674804687500 \n",
      "Minibatch   1800  | loss 5282.841796875000 \n",
      "Minibatch   1900  | loss 75.195236206055 \n",
      "Minibatch   2000  | loss 907.602172851562 \n",
      "Minibatch   2100  | loss 46557.066406250000 \n",
      "Minibatch   2200  | loss 97.939422607422 \n",
      "Minibatch   2300  | loss 112.188796997070 \n",
      "Minibatch   2400  | loss 12.035849571228 \n",
      "Minibatch   2500  | loss  0.411936759949 \n",
      "Minibatch   2600  | loss 1363.206298828125 \n",
      "Minibatch   2700  | loss 30917.398437500000 \n",
      "Minibatch   2800  | loss 3324.807861328125 \n",
      "Minibatch   2900  | loss  2.356801271439 \n",
      "Minibatch   3000  | loss  4.003083229065 \n",
      "Minibatch   3100  | loss  0.010216190480 \n",
      "Minibatch   3200  | loss  0.002227377845 \n",
      "Minibatch   3300  | loss  0.004475673661 \n",
      "Minibatch   3400  | loss  0.065168857574 \n"
     ]
    }
   ],
   "source": [
    "def train(model, data_loaders, optimizer, num_epochs=500, log_every=100, verbose=True):\n",
    "    if CUDA:\n",
    "        model.network.cuda()\n",
    "\n",
    "    data = np.zeros((1000,40))\n",
    "    data_ptr = 0\n",
    "    \n",
    "    iter_ = 0\n",
    "    epoch = 0\n",
    "    if verbose:\n",
    "        print u'Training the model!'\n",
    "        print u'Interrupt at any time to get current model'\n",
    "    try:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            x = data_loaders.get(model, 30)\n",
    "            #data[data_ptr:data_ptr + 30] = x\n",
    "            #data_ptr += 30\n",
    "            #data_ptr %= 1000 - 30\n",
    "            \n",
    "            #random_data_ptr = np.random.randint(0,data_ptr)\n",
    "            #x = data[random_data_ptr : random_data_ptr + 30]\n",
    "            \n",
    "            future = x[:, 21:]\n",
    "            future_scores = model.Q(future)\n",
    "            for i, row in enumerate(future):\n",
    "                #print(row)\n",
    "                game = Threes(save_game=False, data=row.tolist())\n",
    "                for j, move in enumerate(MoveEnum):\n",
    "                    if not game.canMove(move):\n",
    "                        future_scores[i, j] = float('-inf')\n",
    "                if not game.getPossibleMoves():\n",
    "                    future_scores[i:,:] = np.full((1,4), x[i, 20])\n",
    "                \n",
    "            y = x[:, 20] + GAMMA * np.max(future_scores, axis=1)\n",
    "            xx = x[:, :19]\n",
    "            iter_ += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            i = np.asarray(np.vstack((np.arange(0,30),x[:,19])),int)\n",
    "            \n",
    "            i = torch.LongTensor(i)\n",
    "            out = model.Q(xx, as_variable=True)[i[0],i[1]]\n",
    "            loss = model.loss(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter_ % log_every == 0 and verbose:\n",
    "                print u\"Minibatch {0: >6}  | loss {1: >15.12f} \".format(iter_, loss.data[0])\n",
    "            if iter_ % (log_every * 20) == 0:\n",
    "                model.save_parameters(FILENAME)\n",
    "                \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    model.save_parameters(FILENAME)\n",
    "\n",
    "\n",
    "q_learning_net = QLearningNet()\n",
    "for p in q_learning_net.network.parameters():\n",
    "    p.requires_grad = True\n",
    "optimizer = torch.optim.Adam(q_learning_net.network.parameters(), lr=5.000)\n",
    "data_loader = loader.Loader()\n",
    "train(q_learning_net, data_loader, optimizer, num_epochs=400000)\n",
    "print(\"Learning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading net parameters\n",
      "{'Down': 12518, 'Right': 1015, 'Up': 0, 'Left': 37135}\n",
      "387.396 37.788\n"
     ]
    }
   ],
   "source": [
    "from AIModels import *\n",
    "game = Threes(save_game=False)\n",
    "ai = QLearningNetAI(game, FILENAME)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 1000, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 10711, 'Right': 10507, 'Up': 10646, 'Left': 10809}\n",
      "288.54 33.432\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = RandomAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 1000, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 775, 'Right': 741, 'Up': 761, 'Left': 779}\n",
      "12808.5 412.8\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = MiniMaxAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 10, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
