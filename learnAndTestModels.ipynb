{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import loader\n",
    "from threes import *\n",
    "from qLearningNet import *\n",
    "import time\n",
    "from AIModels import *\n",
    "\n",
    "MOVES = [0, 1, 2, 3]\n",
    "CUDA = False\n",
    "\n",
    "FILENAME = \"saved_parameters\"\n",
    "INPUT_SIZE = 19\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.8\n",
    "\n",
    "def testNet(filename=None, net=None):\n",
    "    game = Threes(save_game=False)\n",
    "    if filename is not None:\n",
    "        ai = QLearningNetAI(game, filename=filename)\n",
    "    elif net is not None:\n",
    "        ai = QLearningNetAI(game, net=net)\n",
    "    scores, move_count, highs = AIModel.test_ai(ai, 1000, verbose=False)\n",
    "    return move_count, np.mean(scores), np.mean(highs)\n",
    "\n",
    "\n",
    "# parametry sieci np. learning rate\n",
    "# lepszy zbior uczacy - jak w DeepMindzie\n",
    "# nagrody - musza byc wieksze - inaczej sami sobie redukujemy learning rate\n",
    "# (gestsze nagrody - np. logarytm z rewardow)\n",
    "# uczyc jednak dobrymi przebiegami\n",
    "# http://pytorch.org/docs/master/nn.html#embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model!\n",
      "Interrupt at any time to get current model\n",
      "Minibatch    100  | loss 2248.635986328125 \n",
      "Minibatch    200  | loss  0.000817402033 \n",
      "Minibatch    300  | loss  0.008512672037 \n",
      "Minibatch    400  | loss  0.000948334928 \n",
      "Minibatch    500  | loss  0.001434783218 \n",
      "Minibatch    600  | loss  6.698752880096 \n",
      "Minibatch    700  | loss 300.259094238281 \n",
      "Minibatch    800  | loss 32369.904296875000 \n",
      "Minibatch    900  | loss 70.465797424316 \n",
      "Minibatch   1000  | loss 75.683502197266 \n",
      "Minibatch   1100  | loss 84.878440856934 \n",
      "Minibatch   1200  | loss 60.075538635254 \n",
      "Minibatch   1300  | loss 780.863037109375 \n",
      "Minibatch   1400  | loss 2311.541259765625 \n",
      "Minibatch   1500  | loss 994725.125000000000 \n",
      "Minibatch   1600  | loss 1713.556274414062 \n",
      "Minibatch   1700  | loss 414.998748779297 \n",
      "Minibatch   1800  | loss 10317.613281250000 \n",
      "Minibatch   1900  | loss 76677.664062500000 \n",
      "Minibatch   2000  | loss 32318.750000000000 \n",
      "Minibatch   2100  | loss 1013.955627441406 \n",
      "Minibatch   2200  | loss 1131.960083007812 \n",
      "Minibatch   2300  | loss 1389.116943359375 \n",
      "Minibatch   2400  | loss 127438.554687500000 \n",
      "Minibatch   2500  | loss 175.365661621094 \n",
      "Minibatch   2600  | loss 3121.776367187500 \n",
      "Minibatch   2700  | loss 5189.975097656250 \n",
      "Minibatch   2800  | loss 19941.814453125000 \n",
      "Minibatch   2900  | loss 296.625305175781 \n",
      "Minibatch   3000  | loss 52.394397735596 \n",
      "Minibatch   3100  | loss 2014.023071289062 \n",
      "Minibatch   3200  | loss 12572.261718750000 \n",
      "Minibatch   3300  | loss 626.653747558594 \n",
      "Minibatch   3400  | loss 2313.966552734375 \n",
      "Minibatch   3500  | loss 1873.502319335938 \n",
      "Minibatch   3600  | loss 280777.937500000000 \n",
      "Minibatch   3700  | loss 8450.580078125000 \n",
      "Minibatch   3800  | loss 19.070199966431 \n",
      "Minibatch   3900  | loss  0.016335226595 \n",
      "Minibatch   4000  | loss  0.000037432768 \n",
      "Minibatch   4100  | loss  0.000000311957 \n",
      "Minibatch   4200  | loss  0.000000018787 \n",
      "Minibatch   4300  | loss  0.000000024708 \n",
      "Minibatch   4400  | loss  0.000000127825 \n",
      "Minibatch   4500  | loss  0.000000000001 \n",
      "Minibatch   4600  | loss  0.000000001524 \n",
      "Minibatch   4700  | loss  0.000043560005 \n",
      "Minibatch   4800  | loss  0.760851025581 \n",
      "Minibatch   4900  | loss 72118.281250000000 \n",
      "Minibatch   5000  | loss 197423.359375000000 \n",
      "Minibatch   5100  | loss  7.902920722961 \n",
      "Minibatch   5200  | loss 28.359409332275 \n",
      "Minibatch   5300  | loss 457.224334716797 \n",
      "Minibatch   5400  | loss 39980.023437500000 \n",
      "Minibatch   5500  | loss 675894.750000000000 \n",
      "Minibatch   5600  | loss 1784.052612304688 \n",
      "Minibatch   5700  | loss  0.203700691462 \n",
      "Minibatch   5800  | loss 421.219207763672 \n",
      "Minibatch   5900  | loss  0.000555359933 \n",
      "Minibatch   6000  | loss  0.569288849831 \n",
      "Minibatch   6100  | loss  0.003051335458 \n",
      "Minibatch   6200  | loss  0.121579162776 \n",
      "Minibatch   6300  | loss 269.342346191406 \n",
      "Minibatch   6400  | loss 13879.908203125000 \n",
      "Minibatch   6500  | loss 5949.394042968750 \n",
      "Minibatch   6600  | loss 59.806938171387 \n",
      "Minibatch   6700  | loss  0.142508551478 \n",
      "Minibatch   6800  | loss  0.025197358802 \n",
      "Minibatch   6900  | loss 63.942367553711 \n",
      "Minibatch   7000  | loss 122.097694396973 \n",
      "Minibatch   7100  | loss 113812.031250000000 \n",
      "Minibatch   7200  | loss 3588.725097656250 \n",
      "Minibatch   7300  | loss  2.405738592148 \n",
      "Minibatch   7400  | loss  3.221334218979 \n",
      "Minibatch   7500  | loss  0.000517848239 \n",
      "Minibatch   7600  | loss  0.000459497649 \n",
      "Minibatch   7700  | loss  0.000003129024 \n",
      "Minibatch   7800  | loss  0.000001411170 \n",
      "Minibatch   7900  | loss  0.000002130397 \n",
      "Minibatch   8000  | loss  0.000001013332 \n",
      "Minibatch   8100  | loss  0.009182500653 \n",
      "Minibatch   8200  | loss  2.530986309052 \n",
      "Minibatch   8300  | loss 69779.195312500000 \n",
      "Minibatch   8400  | loss 164.527770996094 \n",
      "Minibatch   8500  | loss  9.477918624878 \n",
      "Minibatch   8600  | loss  0.003658921691 \n",
      "Minibatch   8700  | loss  0.010551766492 \n",
      "Minibatch   8800  | loss  0.000043094919 \n",
      "Minibatch   8900  | loss  0.000000126237 \n",
      "Minibatch   9000  | loss  0.000000001621 \n",
      "Minibatch   9100  | loss  0.000000064174 \n",
      "Minibatch   9200  | loss  0.000000261987 \n",
      "Minibatch   9300  | loss  0.000004804453 \n",
      "Minibatch   9400  | loss  0.000002493763 \n",
      "Minibatch   9500  | loss  0.007620717399 \n",
      "Minibatch   9600  | loss 97.591110229492 \n",
      "Minibatch   9700  | loss 99997.585937500000 \n",
      "Minibatch   9800  | loss 4163.419433593750 \n",
      "Minibatch   9900  | loss 229.974365234375 \n",
      "Minibatch  10000  | loss  0.030664557591 \n",
      "Learning done\n"
     ]
    }
   ],
   "source": [
    "def train(model, data_loaders, optimizer, num_epochs=500, log_every=100, verbose=True):\n",
    "    if CUDA:\n",
    "        model.network.cuda()\n",
    "\n",
    "    data = np.zeros((1000,40))\n",
    "    data_ptr = 0\n",
    "    \n",
    "    epoch = 0\n",
    "    if verbose:\n",
    "        print u'Training the model!'\n",
    "        print u'Interrupt at any time to get current model'\n",
    "    try:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            x = data_loaders.get(model, 30)\n",
    "            #data[data_ptr:data_ptr + 30] = x\n",
    "            #data_ptr += 30\n",
    "            #data_ptr %= 1000 - 30\n",
    "            \n",
    "            #random_data_ptr = np.random.randint(0,data_ptr)\n",
    "            #x = data[random_data_ptr : random_data_ptr + 30]\n",
    "            \n",
    "            future = x[:, 21:]\n",
    "            future_scores = model.Q(future)\n",
    "            for i, row in enumerate(future):\n",
    "                #print(row)\n",
    "                game = Threes(save_game=False, data=row.tolist())\n",
    "                for j, move in enumerate(MoveEnum):\n",
    "                    if not game.canMove(move):\n",
    "                        future_scores[i, j] = float('-inf')\n",
    "                if not game.getPossibleMoves():\n",
    "                    future_scores[i:,:] = np.full((1,4), x[i, 20])\n",
    "                \n",
    "            y = x[:, 20] + GAMMA * np.max(future_scores, axis=1)\n",
    "            xx = x[:, :19]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            i = np.asarray(np.vstack((np.arange(0,30),x[:,19])),int)\n",
    "            \n",
    "            i = torch.LongTensor(i)\n",
    "            out = model.Q(xx, as_variable=True)[i[0],i[1]]\n",
    "            loss = model.loss(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % log_every == 0 and verbose:\n",
    "                print u\"Minibatch {0: >6}  | loss {1: >15.12f} \".format(epoch, loss.data[0])\n",
    "            if epoch % (log_every * 50) == 0:\n",
    "                result = testNet(net=model)\n",
    "                filename = FILENAME +'{}_{}'.format(epoch, int(result[1]))\n",
    "                model.save_parameters(filename)\n",
    "                \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    result = testNet(net=model)\n",
    "    filename = FILENAME +'{}_{}'.format(epoch, int(result[1]))\n",
    "    model.save_parameters(filename)\n",
    "\n",
    "\n",
    "q_learning_net = QLearningNet()\n",
    "for p in q_learning_net.network.parameters():\n",
    "    p.requires_grad = True\n",
    "optimizer = torch.optim.Adam(q_learning_net.network.parameters(), lr=5.000)\n",
    "data_loader = loader.Loader()\n",
    "train(q_learning_net, data_loader, optimizer, num_epochs=10000)\n",
    "print(\"Learning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading net parameters\n",
      "({'Down': 546, 'Right': 19343, 'Up': 6486, 'Left': 159}, 398.58600000000001, 38.676000000000002)\n"
     ]
    }
   ],
   "source": [
    "filename = FILENAME + '6000_447'\n",
    "print testNet(filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 10711, 'Right': 10507, 'Up': 10646, 'Left': 10809}\n",
      "288.54 33.432\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = RandomAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 1000, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 775, 'Right': 741, 'Up': 761, 'Left': 779}\n",
      "12808.5 412.8\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = MiniMaxAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 10, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
