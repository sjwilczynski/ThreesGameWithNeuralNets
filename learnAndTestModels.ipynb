{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import loader\n",
    "from threes import *\n",
    "from qLearningNet import *\n",
    "import time\n",
    "\n",
    "MOVES = [0, 1, 2, 3]\n",
    "CUDA = False\n",
    "\n",
    "FILENAME = \"saved_parameters\"\n",
    "INPUT_SIZE = 19\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.8\n",
    "\n",
    "\n",
    "# parametry sieci np. learning rate\n",
    "# lepszy zbior uczacy - jak w DeepMindzie\n",
    "# nagrody - musza byc wieksze - inaczej sami sobie redukujemy learning rate\n",
    "# (gestsze nagrody - np. logarytm z rewardow)\n",
    "# uczyc jednak dobrymi przebiegami\n",
    "# http://pytorch.org/docs/master/nn.html#embedding \n",
    "# napisac do Adriana po GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model!\n",
      "Interrupt at any time to get current model\n",
      "Minibatch    100  | loss  0.000000132858 \n",
      "Minibatch    200  | loss  0.000000080590 \n",
      "Minibatch    300  | loss  0.000000140362 \n",
      "Minibatch    400  | loss  0.000003429838 \n",
      "Minibatch    500  | loss  0.000006068040 \n",
      "Minibatch    600  | loss  0.000000958801 \n",
      "Minibatch    700  | loss  0.000000036021 \n",
      "Minibatch    800  | loss  0.000253256207 \n",
      "Minibatch    900  | loss  0.000000229461 \n",
      "Minibatch   1000  | loss  0.000001666498 \n",
      "Minibatch   1100  | loss  0.000000706156 \n",
      "Minibatch   1200  | loss  0.000000108188 \n",
      "Minibatch   1300  | loss  0.000002521189 \n",
      "Minibatch   1400  | loss  0.000000749856 \n",
      "Minibatch   1500  | loss  0.000000084256 \n",
      "Minibatch   1600  | loss  0.000000263931 \n",
      "Minibatch   1700  | loss  0.000018225184 \n",
      "Minibatch   1800  | loss  0.000000357961 \n",
      "Minibatch   1900  | loss  0.000002630846 \n",
      "Minibatch   2000  | loss  0.000001175052 \n",
      "Minibatch   2100  | loss  0.000023824326 \n",
      "Minibatch   2200  | loss  0.000000170629 \n",
      "Minibatch   2300  | loss  0.000000789024 \n",
      "Minibatch   2400  | loss  0.000000208256 \n",
      "Minibatch   2500  | loss  0.000000685428 \n",
      "Minibatch   2600  | loss  0.000001837629 \n",
      "Minibatch   2700  | loss  0.000008347184 \n",
      "Minibatch   2800  | loss  0.000017006681 \n",
      "Minibatch   2900  | loss  0.000001706796 \n",
      "Minibatch   3000  | loss  0.000000225002 \n",
      "Minibatch   3100  | loss  0.000001708420 \n",
      "Minibatch   3200  | loss  0.000001079887 \n",
      "Minibatch   3300  | loss  0.000028432791 \n",
      "Minibatch   3400  | loss  0.000004693572 \n",
      "Minibatch   3500  | loss  0.000000142156 \n",
      "Minibatch   3600  | loss  0.000000271479 \n",
      "Minibatch   3700  | loss  0.000004686902 \n",
      "Minibatch   3800  | loss  0.000000560885 \n",
      "Minibatch   3900  | loss  0.000065773442 \n",
      "Minibatch   4000  | loss  0.000032408021 \n",
      "Minibatch   4100  | loss  0.000000820331 \n",
      "Minibatch   4200  | loss  0.000000935413 \n",
      "Minibatch   4300  | loss  0.000002030982 \n",
      "Minibatch   4400  | loss  0.000001623502 \n",
      "Minibatch   4500  | loss  0.000006514193 \n",
      "Minibatch   4600  | loss  0.000000356434 \n",
      "Minibatch   4700  | loss  0.000006500070 \n",
      "Minibatch   4800  | loss  0.000000784645 \n",
      "Minibatch   4900  | loss  0.000002570416 \n",
      "Minibatch   5000  | loss  0.000000286451 \n",
      "Minibatch   5100  | loss  0.000000426708 \n",
      "Minibatch   5200  | loss  0.000008043313 \n",
      "Minibatch   5300  | loss  0.000004889862 \n",
      "Minibatch   5400  | loss  0.000011956658 \n",
      "Minibatch   5500  | loss  0.000003407127 \n",
      "Minibatch   5600  | loss  0.000002679683 \n",
      "Minibatch   5700  | loss  0.000000683477 \n",
      "Minibatch   5800  | loss  0.000004408801 \n",
      "Minibatch   5900  | loss  0.000002199683 \n",
      "Minibatch   6000  | loss  0.000000817477 \n",
      "Minibatch   6100  | loss  0.000002968067 \n",
      "Minibatch   6200  | loss  0.000013186301 \n",
      "Minibatch   6300  | loss  0.000015195431 \n",
      "Minibatch   6400  | loss  0.000002045180 \n",
      "Minibatch   6500  | loss  0.000000196668 \n",
      "Minibatch   6600  | loss  0.000061144528 \n",
      "Minibatch   6700  | loss  0.000000038303 \n",
      "Minibatch   6800  | loss  0.000001601702 \n",
      "Minibatch   6900  | loss  0.000001878049 \n",
      "Minibatch   7000  | loss  0.000000326127 \n",
      "Minibatch   7100  | loss  0.000002340583 \n",
      "Minibatch   7200  | loss  0.000002033502 \n",
      "Minibatch   7300  | loss  0.000001090195 \n",
      "Minibatch   7400  | loss  0.000000102199 \n",
      "Minibatch   7500  | loss  0.000035778550 \n",
      "Minibatch   7600  | loss  0.000000584342 \n",
      "Minibatch   7700  | loss  0.000011317143 \n",
      "Minibatch   7800  | loss  0.000005916493 \n",
      "Minibatch   7900  | loss  0.000001915019 \n",
      "Minibatch   8000  | loss  0.000003994891 \n",
      "Minibatch   8100  | loss  0.000003450468 \n",
      "Minibatch   8200  | loss  0.000003037549 \n",
      "Minibatch   8300  | loss  0.000000554122 \n",
      "Minibatch   8400  | loss  0.000002519225 \n",
      "Minibatch   8500  | loss  0.000005737097 \n",
      "Minibatch   8600  | loss  0.000002061231 \n",
      "Minibatch   8700  | loss  0.000002727339 \n",
      "Minibatch   8800  | loss  0.000004091918 \n",
      "Minibatch   8900  | loss  0.000001014827 \n",
      "Minibatch   9000  | loss  0.000000385407 \n",
      "Minibatch   9100  | loss  0.000000917594 \n",
      "Minibatch   9200  | loss  0.000006548784 \n",
      "Minibatch   9300  | loss  0.000000573404 \n",
      "Minibatch   9400  | loss  0.000004713030 \n",
      "Minibatch   9500  | loss  0.000001738533 \n",
      "Minibatch   9600  | loss  0.000001058942 \n",
      "Minibatch   9700  | loss  0.000002734969 \n",
      "Minibatch   9800  | loss  0.000003061625 \n",
      "Minibatch   9900  | loss  0.000005396094 \n",
      "Minibatch  10000  | loss  0.000000318954 \n",
      "Learning done\n"
     ]
    }
   ],
   "source": [
    "def train(model, data_loaders, optimizer, num_epochs=500, log_every=100, verbose=True):\n",
    "    if CUDA:\n",
    "        model.network.cuda()\n",
    "\n",
    "    data = np.zeros((1000,40))\n",
    "    data_ptr = 0\n",
    "    \n",
    "    iter_ = 0\n",
    "    epoch = 0\n",
    "    if verbose:\n",
    "        print u'Training the model!'\n",
    "        print u'Interrupt at any time to get current model'\n",
    "    try:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            x = data_loaders.get(model, 30)\n",
    "            #data[data_ptr:data_ptr + 30] = x\n",
    "            #data_ptr += 30\n",
    "            #data_ptr %= 1000 - 30\n",
    "            \n",
    "            #random_data_ptr = np.random.randint(0,data_ptr)\n",
    "            #x = data[random_data_ptr : random_data_ptr + 30]\n",
    "            \n",
    "            future = x[:, 21:]\n",
    "            future_scores = model.Q(future)\n",
    "            for i, row in enumerate(future):\n",
    "                #print(row)\n",
    "                game = Threes(save_game=False, data=row.tolist())\n",
    "                for j, move in enumerate(MoveEnum):\n",
    "                    if not game.canMove(move):\n",
    "                        future_scores[i, j] = float('-inf')\n",
    "                if not game.getPossibleMoves():\n",
    "                    future_scores[i:,:] = np.full((1,4), x[i, 20])\n",
    "                \n",
    "            y = x[:, 20] + GAMMA * np.max(future_scores, axis=1)\n",
    "            xx = x[:, :19]\n",
    "            iter_ += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            i = np.asarray(np.vstack((np.arange(0,30),x[:,19])),int)\n",
    "            \n",
    "            i = torch.LongTensor(i)\n",
    "            out = model.Q(xx, as_variable=True)[i[0],i[1]]\n",
    "            loss = model.loss(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter_ % log_every == 0 and verbose:\n",
    "                print u\"Minibatch {0: >6}  | loss {1: >15.12f} \".format(iter_, loss.data[0])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    model.save_parameters(FILENAME)\n",
    "\n",
    "\n",
    "q_learning_net = QLearningNet()\n",
    "for p in q_learning_net.network.parameters():\n",
    "    p.requires_grad = True\n",
    "optimizer = torch.optim.Adam(q_learning_net.network.parameters(), lr=0.001)\n",
    "data_loader = loader.Loader()\n",
    "train(q_learning_net, data_loader, optimizer, num_epochs=10000)\n",
    "print(\"Learning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading net parameters\n",
      "{'Down': 1051, 'Right': 750, 'Up': 824, 'Left': 796}\n",
      "192.15 25.38\n"
     ]
    }
   ],
   "source": [
    "from AIModels import *\n",
    "game = Threes(save_game=False)\n",
    "ai = QLearningNetAI(game, FILENAME)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 100, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 1107, 'Right': 1090, 'Up': 1034, 'Left': 1119}\n",
      "298.26 35.22\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = RandomAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 100, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 775, 'Right': 741, 'Up': 761, 'Left': 779}\n",
      "12808.5 412.8\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = MiniMaxAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 10, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
