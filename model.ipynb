{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import loader\n",
    "from threes import *\n",
    "from qLearningNet import *\n",
    "import time\n",
    "\n",
    "MOVES = [0, 1, 2, 3]\n",
    "CUDA = False\n",
    "\n",
    "FILENAME = \"saved_parameters\"\n",
    "INPUT_SIZE = 19\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model!\n",
      "Interrupt at any time to get current model\n",
      "Minibatch    100  | loss  0.000000620268 \n",
      "Minibatch    200  | loss  0.000000083176 \n",
      "Minibatch    300  | loss  0.000000202193 \n",
      "Minibatch    400  | loss  0.000000003988 \n",
      "Minibatch    500  | loss  0.000000158907 \n",
      "Minibatch    600  | loss  0.000000088219 \n",
      "Minibatch    700  | loss  0.001247189124 \n",
      "Minibatch    800  | loss  0.000000936921 \n",
      "Minibatch    900  | loss  0.000000074460 \n",
      "Minibatch   1000  | loss  0.000000488030 \n",
      "Minibatch   1100  | loss  0.000000766705 \n",
      "Minibatch   1200  | loss  0.000000427949 \n",
      "Minibatch   1300  | loss  0.000007102639 \n",
      "Minibatch   1400  | loss  0.000000115028 \n",
      "Minibatch   1500  | loss  0.000000889642 \n",
      "Minibatch   1600  | loss  0.000001892714 \n",
      "Minibatch   1700  | loss  0.000000497900 \n",
      "Minibatch   1800  | loss  0.000001486336 \n",
      "Minibatch   1900  | loss  0.000000167309 \n",
      "Minibatch   2000  | loss  0.000001648781 \n",
      "Minibatch   2100  | loss  0.000080795064 \n",
      "Minibatch   2200  | loss  0.000000039518 \n",
      "Minibatch   2300  | loss  0.000000274238 \n",
      "Minibatch   2400  | loss  0.000004281496 \n",
      "Minibatch   2500  | loss  0.000000023608 \n",
      "Minibatch   2600  | loss  0.000000067482 \n",
      "Minibatch   2700  | loss  0.000000238763 \n",
      "Minibatch   2800  | loss  0.000029775052 \n",
      "Minibatch   2900  | loss  0.000003164333 \n",
      "Minibatch   3000  | loss  0.000001049085 \n",
      "Minibatch   3100  | loss  0.000005165769 \n",
      "Minibatch   3200  | loss  0.000000609921 \n",
      "Minibatch   3300  | loss  0.000000305407 \n",
      "Minibatch   3400  | loss  0.000002404308 \n",
      "Minibatch   3500  | loss  0.000014361722 \n",
      "Minibatch   3600  | loss  0.000001152380 \n",
      "Minibatch   3700  | loss  0.000000386605 \n",
      "Minibatch   3800  | loss  0.000002020885 \n",
      "Minibatch   3900  | loss  0.000000061295 \n",
      "Minibatch   4000  | loss  0.000042634234 \n",
      "Minibatch   4100  | loss  0.000000323017 \n",
      "Minibatch   4200  | loss  0.000000043936 \n",
      "Minibatch   4300  | loss  0.000093638708 \n",
      "Minibatch   4400  | loss  0.000000642857 \n",
      "Minibatch   4500  | loss  0.000012087045 \n",
      "Minibatch   4600  | loss  0.000000612399 \n",
      "Minibatch   4700  | loss  0.000000554587 \n",
      "Minibatch   4800  | loss  0.000062701576 \n",
      "Minibatch   4900  | loss  0.000004481085 \n",
      "Minibatch   5000  | loss  0.000000131366 \n",
      "Minibatch   5100  | loss  0.000001443411 \n",
      "Minibatch   5200  | loss  0.000005197026 \n",
      "Minibatch   5300  | loss  0.000000622627 \n",
      "Minibatch   5400  | loss  0.000001783532 \n",
      "Minibatch   5500  | loss  0.000001251038 \n",
      "Minibatch   5600  | loss  0.000001837725 \n",
      "Minibatch   5700  | loss  0.000001088441 \n",
      "Minibatch   5800  | loss  0.000007152141 \n",
      "Minibatch   5900  | loss  0.000001268156 \n",
      "Minibatch   6000  | loss  0.000000987343 \n",
      "Minibatch   6100  | loss  0.000006708242 \n",
      "Minibatch   6200  | loss  0.000001093161 \n",
      "Minibatch   6300  | loss  0.000000655358 \n",
      "Minibatch   6400  | loss  0.000005428913 \n",
      "Minibatch   6500  | loss  0.000003538046 \n",
      "Minibatch   6600  | loss  0.000001560578 \n",
      "Minibatch   6700  | loss  0.000003643030 \n",
      "Minibatch   6800  | loss  0.000000711560 \n",
      "Minibatch   6900  | loss  0.000010119276 \n",
      "Minibatch   7000  | loss  0.000002987990 \n",
      "Minibatch   7100  | loss  0.000022263472 \n",
      "Minibatch   7200  | loss  0.000000173828 \n",
      "Minibatch   7300  | loss  0.000001456524 \n",
      "Minibatch   7400  | loss  0.000000573810 \n",
      "Minibatch   7500  | loss  0.000004787442 \n",
      "Minibatch   7600  | loss  0.000006348838 \n",
      "Minibatch   7700  | loss  0.000000685538 \n",
      "Minibatch   7800  | loss  0.000000138407 \n",
      "Minibatch   7900  | loss  0.000002715917 \n",
      "Minibatch   8000  | loss  0.000001042392 \n",
      "Minibatch   8100  | loss  0.000000997875 \n",
      "Minibatch   8200  | loss  0.000007456583 \n",
      "Minibatch   8300  | loss  0.000001364081 \n",
      "Minibatch   8400  | loss  0.000000176787 \n",
      "Minibatch   8500  | loss  0.000011995830 \n",
      "Minibatch   8600  | loss  0.000003644803 \n",
      "Minibatch   8700  | loss  0.000005451803 \n",
      "Minibatch   8800  | loss  0.000002693137 \n",
      "Minibatch   8900  | loss  0.000003231296 \n",
      "Minibatch   9000  | loss  0.000008796507 \n",
      "Minibatch   9100  | loss  0.000002666810 \n",
      "Minibatch   9200  | loss  0.000007165583 \n",
      "Minibatch   9300  | loss  0.000002085139 \n",
      "Minibatch   9400  | loss  0.000000291562 \n",
      "Minibatch   9500  | loss  0.000001409929 \n",
      "Minibatch   9600  | loss  0.000010043727 \n",
      "Minibatch   9700  | loss  0.000008146484 \n",
      "Minibatch   9800  | loss  0.000039161696 \n",
      "Minibatch   9900  | loss  0.000001019432 \n",
      "Minibatch  10000  | loss  0.000002042230 \n",
      "Minibatch  10100  | loss  0.000002844697 \n",
      "Minibatch  10200  | loss  0.000000877040 \n",
      "Minibatch  10300  | loss  0.000001759755 \n",
      "Minibatch  10400  | loss  0.000000229090 \n",
      "Minibatch  10500  | loss  0.000001118501 \n",
      "Minibatch  10600  | loss  0.000091148591 \n",
      "Minibatch  10700  | loss  0.000014554652 \n",
      "Minibatch  10800  | loss  0.000004914731 \n",
      "Minibatch  10900  | loss  0.000000134038 \n",
      "Minibatch  11000  | loss  0.000004442367 \n",
      "Minibatch  11100  | loss  0.000000854055 \n",
      "Minibatch  11200  | loss  0.000008905486 \n",
      "Minibatch  11300  | loss  0.000008154640 \n",
      "Minibatch  11400  | loss  0.000001760080 \n",
      "Minibatch  11500  | loss  0.000005787929 \n",
      "Minibatch  11600  | loss  0.000007035568 \n",
      "Minibatch  11700  | loss  0.000002398890 \n",
      "Minibatch  11800  | loss  0.000000380445 \n",
      "Minibatch  11900  | loss  0.000000484338 \n",
      "Minibatch  12000  | loss  0.000003147665 \n",
      "Minibatch  12100  | loss  0.000000230289 \n",
      "Minibatch  12200  | loss  0.000002099071 \n",
      "Minibatch  12300  | loss  0.000010210410 \n",
      "Minibatch  12400  | loss  0.000000796935 \n",
      "Minibatch  12500  | loss  0.000001325521 \n",
      "Minibatch  12600  | loss  0.000002635914 \n",
      "Minibatch  12700  | loss  0.000001615042 \n",
      "Minibatch  12800  | loss  0.000000792649 \n",
      "Minibatch  12900  | loss  0.000000671403 \n",
      "Minibatch  13000  | loss  0.000007963719 \n",
      "Minibatch  13100  | loss  0.000009700177 \n",
      "Minibatch  13200  | loss  0.000000670317 \n",
      "Minibatch  13300  | loss  0.000000275105 \n",
      "Minibatch  13400  | loss  0.000001156544 \n",
      "Minibatch  13500  | loss  0.000000215594 \n",
      "Minibatch  13600  | loss  0.000000367621 \n",
      "Minibatch  13700  | loss  0.000000439699 \n",
      "Minibatch  13800  | loss  0.000002104605 \n",
      "Minibatch  13900  | loss  0.000000402877 \n",
      "Minibatch  14000  | loss  0.000001273547 \n",
      "Minibatch  14100  | loss  0.000003761260 \n",
      "Minibatch  14200  | loss  0.000000702959 \n",
      "Minibatch  14300  | loss  0.000001957882 \n",
      "Minibatch  14400  | loss  0.000000917660 \n",
      "Minibatch  14500  | loss  0.000003063278 \n",
      "Minibatch  14600  | loss  0.000001067722 \n",
      "Minibatch  14700  | loss  0.000013005976 \n",
      "Minibatch  14800  | loss  0.000001294580 \n",
      "Minibatch  14900  | loss  0.000000500554 \n",
      "Minibatch  15000  | loss  0.000000118319 \n",
      "Minibatch  15100  | loss  0.000003715165 \n",
      "Minibatch  15200  | loss  0.000000807307 \n",
      "Minibatch  15300  | loss  0.000000827918 \n",
      "Minibatch  15400  | loss  0.000000663991 \n",
      "Minibatch  15500  | loss  0.000003259794 \n",
      "Minibatch  15600  | loss  0.000037741094 \n",
      "Minibatch  15700  | loss  0.000010003834 \n",
      "Minibatch  15800  | loss  0.000000685696 \n",
      "Minibatch  15900  | loss  0.000001066588 \n",
      "Minibatch  16000  | loss  0.000001851905 \n",
      "Minibatch  16100  | loss  0.000000406190 \n",
      "Minibatch  16200  | loss  0.000002829004 \n",
      "Minibatch  16300  | loss  0.000002436219 \n",
      "Minibatch  16400  | loss  0.000000973721 \n",
      "Minibatch  16500  | loss  0.000000469725 \n",
      "Minibatch  16600  | loss  0.000001896565 \n",
      "Minibatch  16700  | loss  0.000000363614 \n",
      "Minibatch  16800  | loss  0.000002101653 \n",
      "Minibatch  16900  | loss  0.000000689759 \n",
      "Minibatch  17000  | loss  0.000001071782 \n",
      "Minibatch  17100  | loss  0.000000151566 \n",
      "Minibatch  17200  | loss  0.000002025911 \n",
      "Minibatch  17300  | loss  0.000000365317 \n",
      "Minibatch  17400  | loss  0.000001252113 \n",
      "Minibatch  17500  | loss  0.000001039355 \n",
      "Minibatch  17600  | loss  0.000001243160 \n",
      "Minibatch  17700  | loss  0.000009137196 \n",
      "Minibatch  17800  | loss  0.000000637218 \n",
      "Minibatch  17900  | loss  0.000001796098 \n",
      "Minibatch  18000  | loss  0.000000222463 \n",
      "Minibatch  18100  | loss  0.000000360579 \n",
      "Minibatch  18200  | loss  0.000000231412 \n",
      "Minibatch  18300  | loss  0.000002141523 \n",
      "Minibatch  18400  | loss  0.000000156982 \n",
      "Minibatch  18500  | loss  0.000000238589 \n",
      "Minibatch  18600  | loss  0.000000502714 \n",
      "Minibatch  18700  | loss  0.000001464912 \n",
      "Minibatch  18800  | loss  0.000000364276 \n",
      "Minibatch  18900  | loss  0.000000671756 \n",
      "Minibatch  19000  | loss  0.000001064566 \n",
      "Minibatch  19100  | loss  0.000000768143 \n",
      "Minibatch  19200  | loss  0.000000669984 \n",
      "Minibatch  19300  | loss  0.000001052569 \n",
      "Minibatch  19400  | loss  0.000000172266 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch  19500  | loss  0.000001366352 \n",
      "Minibatch  19600  | loss  0.000000153711 \n",
      "Minibatch  19700  | loss  0.000001644709 \n",
      "Minibatch  19800  | loss  0.000001371313 \n",
      "Minibatch  19900  | loss  0.000000290862 \n",
      "Minibatch  20000  | loss  0.000000137002 \n",
      "Minibatch  20100  | loss  0.000000247624 \n",
      "Minibatch  20200  | loss  0.000001403168 \n",
      "Minibatch  20300  | loss  0.000000379105 \n",
      "Minibatch  20400  | loss  0.000002590389 \n",
      "Minibatch  20500  | loss  0.000002901296 \n",
      "Minibatch  20600  | loss  0.000003586148 \n",
      "Minibatch  20700  | loss  0.000000154050 \n",
      "Minibatch  20800  | loss  0.000001100864 \n",
      "Minibatch  20900  | loss  0.000000313443 \n",
      "Minibatch  21000  | loss  0.000000264703 \n",
      "Minibatch  21100  | loss  0.000000470014 \n",
      "Minibatch  21200  | loss  0.000002218024 \n",
      "Minibatch  21300  | loss  0.000001323432 \n",
      "Minibatch  21400  | loss  0.000003518844 \n",
      "Minibatch  21500  | loss  0.000000329361 \n",
      "Minibatch  21600  | loss  0.000000034390 \n",
      "Minibatch  21700  | loss  0.000002082264 \n",
      "Minibatch  21800  | loss  0.000000193722 \n",
      "Minibatch  21900  | loss  0.000000145851 \n",
      "Minibatch  22000  | loss  0.000000609928 \n",
      "Minibatch  22100  | loss  0.000001284182 \n",
      "Minibatch  22200  | loss  0.000000593971 \n",
      "Minibatch  22300  | loss  0.000001002843 \n",
      "Minibatch  22400  | loss  0.000001775143 \n",
      "Minibatch  22500  | loss  0.000000647449 \n",
      "Minibatch  22600  | loss  0.000004568721 \n",
      "Minibatch  22700  | loss  0.000000375362 \n",
      "Minibatch  22800  | loss  0.000000431809 \n",
      "Minibatch  22900  | loss  0.000001557255 \n",
      "Minibatch  23000  | loss  0.000000050718 \n",
      "Minibatch  23100  | loss  0.000000284262 \n",
      "Minibatch  23200  | loss  0.000000120956 \n",
      "Minibatch  23300  | loss  0.000000328212 \n",
      "Minibatch  23400  | loss  0.000000144242 \n",
      "Minibatch  23500  | loss  0.000000140529 \n",
      "Minibatch  23600  | loss  0.000000141150 \n",
      "Minibatch  23700  | loss  0.000000369989 \n",
      "Minibatch  23800  | loss  0.000000358714 \n",
      "Minibatch  23900  | loss  0.000000329314 \n",
      "Minibatch  24000  | loss  0.000000685789 \n",
      "Minibatch  24100  | loss  0.000000330070 \n",
      "Minibatch  24200  | loss  0.000000324278 \n",
      "Minibatch  24300  | loss  0.000001158703 \n",
      "Minibatch  24400  | loss  0.000000687354 \n",
      "Minibatch  24500  | loss  0.000000072156 \n",
      "Minibatch  24600  | loss  0.000000674826 \n",
      "Minibatch  24700  | loss  0.000004074475 \n",
      "Minibatch  24800  | loss  0.000000110453 \n",
      "Minibatch  24900  | loss  0.000000101955 \n",
      "Minibatch  25000  | loss  0.000002348120 \n",
      "Learning done\n"
     ]
    }
   ],
   "source": [
    "def train(model, data_loaders, optimizer, num_epochs=500, log_every=100, verbose=True):\n",
    "    if CUDA:\n",
    "        model.network.cuda()\n",
    "\n",
    "    data = np.zeros((1000,40))\n",
    "    data_ptr = 0\n",
    "    \n",
    "    iter_ = 0\n",
    "    epoch = 0\n",
    "    if verbose:\n",
    "        print u'Training the model!'\n",
    "        print u'Interrupt at any time to get current model'\n",
    "    try:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            x = data_loaders.get(model, 30)\n",
    "            #data[data_ptr:data_ptr + 30] = x\n",
    "            #data_ptr += 30\n",
    "            #data_ptr %= 1000 - 30\n",
    "            \n",
    "            #random_data_ptr = np.random.randint(0,data_ptr)\n",
    "            #x = data[random_data_ptr : random_data_ptr + 30]\n",
    "            \n",
    "            future = x[:, 21:]\n",
    "            future_scores = model.Q(future)\n",
    "            for i, row in enumerate(future):\n",
    "                #print(row)\n",
    "                game = Threes(save_game=False, data=row.tolist())\n",
    "                for j, move in enumerate(MoveEnum):\n",
    "                    if not game.canMove(move):\n",
    "                        future_scores[i, j] = float('-inf')\n",
    "                if not game.getPossibleMoves():\n",
    "                    future_scores[i:,:] = np.full((1,4), x[i, 20])\n",
    "                \n",
    "            y = x[:, 20] + GAMMA * np.max(future_scores, axis=1)\n",
    "            xx = x[:, :19]\n",
    "            iter_ += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            i = np.asarray(np.vstack((np.arange(0,30),x[:,19])),int)\n",
    "            \n",
    "            i = torch.LongTensor(i)\n",
    "            out = model.Q(xx, as_variable=True)[i[0],i[1]]\n",
    "            loss = model.loss(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter_ % log_every == 0 and verbose:\n",
    "                print u\"Minibatch {0: >6}  | loss {1: >15.12f} \".format(iter_, loss.data[0])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    model.save_parameters(FILENAME)\n",
    "\n",
    "\n",
    "q_learning_net = QLearningNet()\n",
    "for p in q_learning_net.network.parameters():\n",
    "    p.requires_grad = True\n",
    "optimizer = torch.optim.Adam(q_learning_net.network.parameters(), lr=0.001)\n",
    "data_loader = loader.Loader()\n",
    "train(q_learning_net, data_loader, optimizer, num_epochs=25000)\n",
    "print(\"Learning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 489, 'Right': 1090, 'Up': 873, 'Left': 876}\n",
      "181.53 23.46\n"
     ]
    }
   ],
   "source": [
    "from AIModels import *\n",
    "game = Threes(save_game=False)\n",
    "ai = QLearningNetAI(game, FILENAME)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 100, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 1107, 'Right': 1090, 'Up': 1034, 'Left': 1119}\n",
      "298.26 35.22\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = RandomAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 100, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 775, 'Right': 741, 'Up': 761, 'Left': 779}\n",
      "12808.5 412.8\n"
     ]
    }
   ],
   "source": [
    "game = Threes(save_game=False)\n",
    "ai = MiniMaxAI(game)\n",
    "scores, move_count, highs = AIModel.test_ai(ai, 10, verbose=False)\n",
    "print move_count\n",
    "print np.mean(scores), np.mean(highs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
